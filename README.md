# nano-gpt-copy

This project implements a Bigram Language Model using PyTorch. The model can be trained on text data and used to generate text based on learned patterns.

## Features
- Train a bigram-based language model on a given text dataset.
- Generate text using a trained model.
- Command-line interface for easy usage.

## Installation
Ensure you have Python and the required dependencies installed:
```bash
pip install -r requirements.txt
```

## Usage

### Training the Model
To train the model, run:
```bash
python src/main.py --mode train --model_path models/bigram_model.pth --max_iters 5000
```
- `--mode train` : Specifies training mode.
- `--model_path` : Path to save or load the model.
- `--max_iters` : Number of training iterations.

### Generating Text
To generate text using a trained model, run:
```bash
python src/main.py --mode generate --model_path models/bigram_model.pth
```
- `--mode generate` : Specifies text generation mode.
- `--model_path` : Path to the trained model.

## Project Structure
```
.
â”œâ”€â”€ .venv/                 # Virtual environment (optional)
â”œâ”€â”€ models/                # Directory to store trained models
â”œâ”€â”€ src/                   # Source code directory
â”‚   â”œâ”€â”€ main.py            # Main script for training and generating text
â”‚   â”œâ”€â”€ bigram_model.py    # Bigram language model implementation
â”‚   â”œâ”€â”€ train.py           # Training function
â”œâ”€â”€ input.txt              # Input text file for training 
â”œâ”€â”€ LICENSE                # Project license
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ requirements.txt       # Dependencies
```

## Input Data
The input text file is based on the [Tiny Shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)
 dataset:

## Acknowledgment
This project is inspired by and based on the work of [Andrej Karpathy](https://karpathy.ai/). A huge appreciation for his contributions to deep learning and open-source education! ðŸ™Œ

